---
tutorial_type: distributed
source: samtools_count_distr_region_py
language: python
title: SAMtools count scatter gather based on regions
---
# SAMtools count distributed by Chromosome (DNAnexus Platform App)

Distributed count of reads in BAM format file. Documentation to create a distributed applet can be found on the [DNAnexus wiki](https://wiki.dnanexus.com/Developer-Tutorials/Parallelize-Your-App). This readme will focus on the details of this applet.

<hr>## How is SAMtools dependency provided?
SAMtools dependency is resolved by declaring an [Apt-Get](https://help.ubuntu.com/14.04/serverguide/apt-get.html) package in the dxapp.json runSpec.execDepends.
```
  "runSpec": {
    ...
    "execDepends": [
      {"name": "samtools"}
    ]
  }
```
For additional information, please refer to the [execDepends wiki page](https://wiki.dnanexus.com/Execution-Environment-Reference#Software-Packages).

<hr>## Entry points
Distributed python-interpreter apps use python decorators on functions to [declare entry points](https://wiki.dnanexus.com/Developer-Tutorials/Parallelize-Your-App#Adding-Entry-Points-to-Your-Code). This app has the following entry points as decorated functions:

* *main* 
* *samtoolscount_bam*
* *combine_files*

Entry points are executed on a new worker with their own system requirements. In this example, we *split* and *merge* our files on basic mem1_ssd1_x2 instsances and perform our, more intensive, *processing* step on a mem1_ssd1_x4 instance. Instance type can be set in the dxapp.json `runSpec.systemRequirements`:
```
  "runSpec": {
    ...
    "systemRequirements": {
      "main": {
        "instanceType": "mem1_ssd1_x2"
      },
      "samtoolscount_bam": {
        "instanceType": "mem1_ssd1_x4"
      },
      "combine_files": {
        "instanceType": "mem1_ssd1_x2"
      }
    },
    ...
  }
```
<hr>## Overview
### main
The *main* function bins by region number based on user input. If no `*.bai` file is present, The applet generates an index `*.bai`. Regions bins are passed to the *samtoolscount_bam* entry point using [dxpy.new_dxjob](http://autodoc.dnanexus.com/bindings/python/current/dxpy_apps.html?highlight=new_dxjob#dxpy.bindings.dxjob.new_dxjob) function.

Outputs from the *samtoolscount_bam* entry points are used as inputs for the *combine_files* entry point.

The output of the *combine_files* entry point is used as the output of the main entry point.

### samtoolscount_bam
This entry point downloads and creates a `samtools view -c` command for each region in the input bin. The dictionary returned from `dxpy.download_all_inputs()` is used to reference input names and paths.

### combine_files
The *main* entry point triggers this sub job, providing the output of *samtoolscount_bam* as an input. This entry point gathers all the files generated by the *samtoolscount_bam* jobs and sums them.
<hr>
## Applet Script

```python
import os
import dxpy
import subprocess
import shutil
import re


class NotIndexedException(Exception):
    pass


def create_region_view_cmd(input_bam, region):
    view_cmd = ['samtools', 'view', '-c', input_bam, region]
    return view_cmd


def run_cmd(cmd_arr):
    proc = subprocess.Popen(
        cmd_arr,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE)
    stdout, stderr = proc.communicate()
    exit_code = proc.returncode
    if exit_code != 0:
        raise subprocess.CalledProcessError(
            returncode=exit_code,
            cmd=" ".join(cmd_arr),
            output=stdout)
    elif 'is not sorted' in stderr:
        raise NotIndexedException("BAM file is not indexed")
    proc_tuple = (stdout, stderr, exit_code)
    return proc_tuple


def create_index_file(bam_filename, bam_dxlink):

    Returns:
        regions (list[string]): List of regions in BAM header

    Arguments:
        line_read (str): header line from BAM file.

    Returns:
        Integer for the amount of reads.

    Arguments:
        region_list (list[str]): Regions to count in BAM
        mappings_bam (dict): dxlink to input BAM
        index_file (dict): dxlink to input BAM

    Returns:
        Dictionary containing dxlinks to the uploaded read counts file

    Arguments:
        countDXlinks (list[dict]): list of DXlinks to process job output files.
        resultfn (str): Filename to use for job output file.

    Returns:
        DXLink for the main function to return as the job output.

    Note: Only the DXLinks are passed as parameters.
    Subjobs work on a fresh instance so files must be downloaded to the machine

    The main function will perform logic to distribute our job
    across multiple workers (instances)

    Returns:
        output (dict): Contains key "count_file" with value DXLink to job output file.
```
