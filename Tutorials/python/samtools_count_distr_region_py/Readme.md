The applet will create a count of reads from a BAM format file. Documentation to create a distributed applet can be found on the [DNAnexus documentation site](https://documentation.dnanexus.com/faqs/developing-apps-and-applets#how-do-i-parallelize-my-app). This readme will focus on the details of this applet.

## How is the SAMtools dependency provided?
The SAMtools dependency is resolved by declaring an [Apt-Get](http://manpages.ubuntu.com/manpages/xenial/man8/apt-get.8.html) package in the `dxapp.json` `runSpec.execDepends`.
```json
  "runSpec": {
    ...
    "execDepends": [
      {"name": "samtools"}
    ]
  }
```
For additional information, please refer to the [`execDepends` documentation](https://documentation.dnanexus.com/developer/api/running-analyses/io-and-run-specifications#run-specification).

## Entry Points
Distributed python-interpreter apps use python decorators on functions to declare entry points. This app has the following entry points as decorated functions:

* *main*
* *samtoolscount_bam*
* *combine_files*

Entry points are executed on a new worker with their own system requirements. In this example, we *split* and *merge* our files on basic mem1_ssd1_x2 instances and perform our own, more intensive, *processing* step on a mem1_ssd1_x4 instance. Instance type can be set in the dxapp.json `runSpec.systemRequirements`:
```json
  "runSpec": {
    ...
    "systemRequirements": {
      "main": {
        "instanceType": "mem1_ssd1_x2"
      },
      "samtoolscount_bam": {
        "instanceType": "mem1_ssd1_x4"
      },
      "combine_files": {
        "instanceType": "mem1_ssd1_x2"
      }
    },
    ...
  }
```
## main
The *main* function scatters by region bins based on user input. If no `*.bai` file is present, the applet generates an index `*.bai`.
<!-- SECTION: Scatter -->
Regions bins are passed to the *samtoolscount_bam* entry point using the [`dxpy.new_dxjob`](http://autodoc.dnanexus.com/bindings/python/current/dxpy_apps.html?highlight=new_dxjob#dxpy.bindings.dxjob.new_dxjob) function.
<!-- SECTION: Processing -->
Outputs from the *samtoolscount_bam* entry points are used as inputs for the *combine_files* entry point. The output of the *combine_files* entry point is used as the output of the main entry point.
<!-- SECTION: Gather (Post-processing) -->

## samtoolscount_bam
This entry point downloads and creates a `samtools view -c` command for each region in the input bin. The dictionary returned from `dxpy.download_all_inputs()` is used to reference input names and paths.
<!-- FUNCTION: samtoolscount_bam -->
This entry point returns `{"readcount_fileDX": readCountDXlink}`, a JBOR referencing an uploaded text file. This approach to scatter-gather stores the results in files and uploads/downloads the information as needed. This approach exaggerates a scatter-gather for tutorial purposes. You're able to pass types other than **file** such as **int**.
## combine_files
The *main* entry point triggers this subjob, providing the output of *samtoolscount_bam* as an input. This entry point gathers all the files generated by the *samtoolscount_bam* jobs and sums them.
<!-- FUNCTION: combine_files -->
<!-- INCLUDE: {% include important.html content="While the _main_ entry point triggers the _processing_ and _gathering_ entry points, keep in mind the _main_ entry point **doesn't** do any heavy lifting or _processing_. Notice in the `.runSpec` json [above](#Entry-Points) we start with a lightwieght instance, _scale up_ for the processing entry point, then finally _scale down_ for the _gathering_ step." %} -->
