#!/usr/bin/env python
#
# Copyright (C) 2013 DNAnexus, Inc.
#   This file is part of dnanexus-example-applets.
#   You may use this file under the terms of the Apache License, Version 2.0;
#   see the License.md file for more information.


# import_sam_to_visualize 0.0.1
# Generated by dx-app-wizard.
#
# Parallelized execution pattern: Your app will generate multiple jobs
# to perform some computation in parallel, followed by a final
# "postprocess" stage that will perform any additional computations as
# necessary.
#
# See http://wiki.dnanexus.com/Developer-Tutorials/Intro-to-Building-Apps
# for instructions on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import os
import dxpy

import subprocess
import re
import string

@dxpy.entry_point('postprocess')
def postprocess(process_outputs):
    # Change the following to process whatever input this stage
    # receives.  You may also want to copy and paste the logic to download
    # and upload files here as well if this stage receives file input
    # and/or makes file output.

    for output in process_outputs:
        table = dxpy.open_dxgtable(output)
        table.close()
        break

    return { "answer": None }

@dxpy.entry_point('process')
def process(tableId, BAM, job_id, chunks):
    # Change the following to process whatever input this stage
    # receives.  You may also want to copy and paste the logic to download
    # and upload files here as well if this stage receives file input
    # and/or makes file output.

    BAM = dxpy.DXFile(BAM)
    dxpy.download_dxfile(BAM.get_id(), "subset.bam")

    subprocess.check_call("samtools view subset.bam -F 4 -F 1024 | parseSam %s %d %d" % (tableId, job_id, chunks), shell=True)

    complement_table = string.maketrans("ATGCatgc", "TACGtacg")


    return { "output": tableId }

@dxpy.entry_point('main')
def main(BAM, reference, mb_per_chunk):

    # The following line(s) initialize your data object inputs on the platform
    # into dxpy.DXDataObject instances that you can start using immediately.

    #BAM = dxpy.DXFile(BAM)

    # The following line(s) download your file inputs to the local file system
    # using variable names for the filenames.

    #dxpy.download_dxfile(BAM.get_id(), "input.bam")

    # Split your work into parallel tasks.  As an example, the
    # following generates 10 subjobs running with the same dummy
    # input.

    chunks = int(dxpy.DXFile(BAM).describe()['size']/(1000000*mb_per_chunk))
    if chunks < 1:
        chunks = 1

    #subprocess.check_call("samtools view input.bam -H -o header.txt", shell=True)
    #chromosomes  = re.findall("SN:([^\t]*)", line.strip())

    schema = [
        {"name": "sequence", "type":"string"},
        {"name": "chr", "type": "string"},
        {"name": "lo", "type": "int32"},
        {"name": "hi", "type": "int32"},
        {"name": "negative_strand", "type": "boolean"},
        {"name": "cigar", "type": "string"}
         ]

    mappingsTable = dxpy.new_dxgtable(schema, indices=[dxpy.DXGTable.genomic_range_index("chr", "lo", "hi", "gri")])
    mappingsTable.add_types(["Mappings"])

    mappingsTable.set_details({"original_contigset":dxpy.dxlink(reference)})

    subjobs = []
    for i in range(chunks):

        #subprocess.check_call("samtools view -b input.bam -F 4 -o subset.bam %s" % (" ".join(chromosomes[i::chunks])), shell=True)
        #jobFile = dxpy.upload_local_file("subset.bam").get_id()

        subjob_input = { "tableId": mappingsTable.get_id(),
                         "BAM": BAM,
                         "job_id": i,
                         "chunks": chunks}
        subjobs.append(dxpy.new_dxjob(subjob_input, 'process'))

    # The following line creates the job that will perform the
    # "postprocess" step of your app.  If you give it any inputs that
    # use outputs from the "process" jobs, then it will automatically
    # wait for those jobs to finish before it starts running.  If you
    # do not need to give it any such inputs, you can explicitly state
    # the dependencies to wait for those jobs to finish by setting the
    # "depends_on" field to the list of subjobs to wait for (it
    # accepts either DXJob objects are string job IDs in the list).

    postprocess_job = dxpy.new_dxjob(fn_input={ "process_outputs": [subjob.get_output_ref("output") for subjob in subjobs] },
                                     fn_name='postprocess',
                                     depends_on=subjobs)

    # If you would like to include any of the output fields from the
    # postprocess_job as the output of your app, you should return it
    # here using a reference.  If the output field in the postprocess
    # function is called "answer", you can pass that on here as
    # follows:
    #
    # return { "app_output_field": postprocess_job.get_output_ref("answer"), ...}

    output = {'mappings': dxpy.dxlink(mappingsTable.get_id())}

    return output

dxpy.run()
